#  Rearchitecting Linux Storage Stack for µs Latency and High Throughput Jaehyun Hwang and Midhul Vuppalapati, Cornell University;
## Simon Peter, UT Austin; Rachit Agarwal, Cornell University
https://www.usenix.org/conference/osdi21/presentation/hwang


### Abstract

这篇论文展示了在Linux内核存储栈中实现微秒级延迟的可行性，即使有大量对延迟敏感的应用程序与吞吐量受限的读/写操作应用程序竞争主机资源。更重要的是，即使不对应用程序、网络硬件、内核CPU调度器和/或内核网络栈进行任何修改，也能实现这一性能。作者通过设计了一个名为blk-switch的新Linux内核存储栈架构来证明这一观点。blk-switch的关键见解在于，Linux的多队列存储设计与多队列网络和存储硬件使存储栈在概念上类似于网络交换机。blk-switch通过应用计算机网络文献中的技术，如多个出口队列、个别请求的优先处理、负载平衡和交换机调度，成功地将这些概念引入Linux内核存储栈。blk-switch在各种场景下的评估表明，它能够始终实现微秒级的平均延迟和尾延迟（在第99和99.9个百分位数上），同时允许应用程序充分利用硬件容量。


**********************************************
### 1 Introduction
首先讨论了普遍认为在Linux内核栈中无法实现微秒级尾延迟的观点，并指出这一观点的原因包括Linux的高CPU开销，与存储和网络硬件性能提升的不匹配，以及在多租户环境中的性能问题。论文的目标是通过重新设计Linux存储栈来推翻这一观点。

blk-switch是一个新的Linux存储栈架构，通过利用Linux的多队列存储设计和现代多队列硬件，将存储栈的概念与网络交换机类比。blk-switch借鉴了计算机网络文献中的技术，如多个出口队列、优先处理、负载平衡和交换机调度，将这些概念引入Linux内核存储栈。

在blk-switch的设计中，引入了两个关键机制：请求引导和应用程序引导。请求引导通过将L应用程序的请求映射到出口队列并按优先级处理，以确保L应用程序观察最小的延迟增加。应用程序引导则通过在较粗粒度的时间尺度上引导应用程序线程到可用的核心，以避免L应用程序和T应用程序之间的持久性争用。

实现了blk-switch并在各种场景下进行了广泛评估，包括不同存储介质、单线程和多线程应用程序、不同负载情况等。评估结果表明，在除核心数量和T应用程序负载敏感性分析外的所有场景中，blk-switch都能够实现微秒级平均和尾延迟，并使应用程序几乎饱和地利用100Gbps链路容量。与Linux相比，blk-switch在提高延迟的同时保持了高吞吐量，甚至在与SPDK等先进用户空间存储栈的比较中，blk-switch也取得了显著的性能优势。

blk-switch通过重新架构Linux存储栈，成功地实现了对L应用程序的低延迟和对T应用程序的高吞吐量，同时在多个方面超越了传统的Linux和其他存储栈的性能。


************************************************
### 2 Understanding Existing Storage Stacks
深入研究了两个先进的存储栈，Linux和SPDK。关键发现包括：

尽管Linux存储栈进行了改进，但在多租户环境中，尾延迟受到头阻塞的影响，导致性能问题。
基于轮询的存储栈（如SPDK）在每个应用程序有专用核心时表现良好，但在多个应用程序共享核心时与内核CPU调度器交互导致性能下降。

* 2.1 Measurement Setup
提供了实验设置的概要，包括评估系统中使用的存储栈、网络栈和CPU调度器。Linux采用基于块的多队列设计，而SPDK是一个基于轮询的系统。实验重点在单核心设置下，其中一个T应用程序与逐渐增多的L应用程序竞争执行远程内存存储的读请求。通过测量平均和P99尾延迟以及T应用程序的每核吞吐量，评估了隔离和共享情景下系统性能。理想情况下，系统应在L应用程序的隔离延迟上保持，并对T应用程序的隔离吞吐量影响最小。

* 2.2 Existing Storage Stacks: Low latency or
high throughput, but not both
这一部分讨论了两个存储栈系统的隔离性能，Linux和SPDK。在隔离状态下，Linux具有118µs的P99尾延迟和每核26Gbps的吞吐量，而SPDK在延迟上表现更好，吞吐量也更高。然而，Linux存在高尾延迟的问题，主要原因包括由于缺乏优先级而导致的头阻塞和CPU调度器的公平性问题。基于轮询的设计与默认内核CPU调度器（完全公平调度器）不良地相互作用，导致延迟膨胀和T应用程序吞吐降低。优先级设计也不能解决问题，可能导致T应用程序完全饥饿。实验结果显示，提高L应用程序的优先级会增加其吞吐量，但以L应用程序的尾延迟为代价。最后，调整L应用程序的休眠间隔和优先级也对系统性能产生影响。


************************************************
### 3 blk-switch Design
blk-switch基于Linux的每个核心块层队列和现代多队列存储、网络硬件的观点，将Linux存储栈概念上类比于网络交换机。该设计引入了一种“切换”架构，允许应用程序提交的请求在系统中的任何核心中导向和处理，从而实现低延迟。通过解耦应用程序端队列和设备端队列，并通过切换架构相互连接，blk-switch实现了不同的负载平衡策略，以实现T应用程序的高吞吐量。值得注意的是，blk-switch的设计不需要对应用程序或系统接口进行修改，使用ionice接口识别应用程序目标，支持易扩展的应用程序要求。


* 3.1 Block Layer is the New Switch
讨论了Linux存储栈架构中块层的演进，特别是块层的多核队列（blk-mq [19]）在当前Linux中的应用。介绍了blk-switch的“切换”架构，通过在Linux存储栈引入多核块层架构，避免了由于驱动程序队列中的头阻塞而导致的高尾延迟。blk-switch引入了每核多出口队列的概念，通过为服务器上运行的每类应用程序创建一个“出口”队列，将这些队列映射到底层设备驱动程序的唯一队列。blk-switch利用多出口队列的设计，实现了请求处理与应用程序核心解耦的目标，允许请求在系统中的任何核心中处理。此外，blk-switch通过为每个出口队列分配优先级，并在内核CPU调度器中优化处理线程，有效地实现了对T应用程序的高吞吐量。这一节详细描述了blk-switch的设计机制，以实现对切换架构的高效利用，以实现T应用程序的高吞吐量。

* 3.2 Request Steering
提供了blk-switch在Linux存储栈中的实现细节。首先描述了blk-switch与现有Linux存储栈组件的接口，然后讨论了为实现blk-switch的切换架构和请求转向机制所做的修改。

** 4.1 与Linux存储栈的交互**
blk-switch与Linux存储栈的多个组件进行了集成，包括块层、网络栈和CPU调度器。关键交互如下：
1. **块层集成：** 利用Linux块层的多队列（blk-mq）设计，为不同应用类别创建和管理每个核心的入口和出口队列。
2. **网络栈集成：** 使用标准的Linux网络栈进行核心间通信，通过内核TCP/IP堆栈实现核心间请求的发送和接收。
3. **CPU调度器交互：** 使用默认的Linux完全公平调度器（CFS），利用ionice接口设置应用程序或进程的“调度类别”以区分延迟敏感和吞吐量受限的应用程序。
4. **请求转向通信：** 通过核心间消息传递，实现请求转向时各核心之间的信息交换。

** 4.2 切换架构修改**
切换架构的实现涉及对现有块层设计的修改，以支持每个核心的入口和出口队列。关键修改包括：
1. **入口队列：** 引入每个核心的入口队列，利用blk-mq基础设施确保适当的入口队列创建和管理。
2. **出口队列：** 创建不同应用类别的每个核心的出口队列，映射到底层存储设备的驱动程序队列。
3. **请求转向逻辑：** 在块层的请求提交路径上进行修改，增加逻辑以检查本地核心的负载并决定是本地入队还是转向其他核心。
4. **核心间通信：** 为支持请求转向，建立核心之间的通信通道，包括消息机制、适当的锁和同步机制，确保负载信息交换的一致性。
5. **线程优先级：** 为处理出口队列的分配专用内核线程是切换架构的一部分，基于应用程序类别设置这些线程的优先级。

** 4.3 请求转向实现**
请求转向机制的实现涉及多个组件，包括负载估计、决策逻辑、执行过程和协调机制：
1. **负载估计：** 通过监视出口队列中未完成请求的瞬时字节总和，blk-switch估计每个核心上的负载。
2. **决策逻辑：** 决策逻辑根据负载估计确定是否进行请求转向。如果本地核心过载，逻辑使用二次幂选择从目标核心中选择一个进行转向。
3. **执行过程：** 一旦做出决策，blk-switch执行请求转向过程，将请求从本地出口队列重定向到选择的目标核心的入口队列。
4. **协调机制：** 为了协调请求转向的核心间通信，blk-switch使用适当的锁和同步机制，确保多个核心可以在没有冲突的情况下交换负载信息和做出请求转向决策。

请求转向实现涉及对块层请求提交和处理路径的更改，以及引入核心间通信和协调机制。

*****************************************************************
### 4 blk-switch Implementation Details
在Linux内核5.4中实现了blk-switch，致力于最大限度地重用现有的内核存储栈基础设施。总的来说，实现添加了928行代码，分别涉及blk-mq层（530行）、设备驱动程序层（118行）和远程存储层的目标特定功能（280行）。以下是核心组件和实现细节：

Linux块层概述：详细描述了Linux存储栈如何使用异步I/O接口。在创建I/O请求之前，应用程序需要通过io_setup()设置I/O上下文。这个过程创建了一个kioctx结构，包括一个环形缓冲区和应用程序进程信息。每个kioctx与上下文标识符相关联。通过提交请求，VFS层创建kiocb，表示I/O请求，并通过标识符找到相应的kioctx。然后，块层创建一个基于kiocb的bio实例，并将其封装在请求实例中。请求实例包括与设备驱动程序I/O队列相关的硬件上下文（hctx）。在将请求发送到设备驱动程序队列之前，需要获取标签号。块层维护标签的位图，以跟踪其占用情况。在I/O处理后，响应带有相同的标签号返回内核，然后通过标签找到请求实例，最终将完成事件通知应用程序。

blk-switch请求转向实现： 每个hctx被视为出口队列，请求转向的目标是在本地队列拥塞时选择非拥塞的hctx。blk-switch通过维护每核负载实现请求转向。一旦确定目标hctx，请求将获得标签并排队到相应的驱动程序队列。驱动程序和块层接收处理将在关联的核心上执行。当设备返回响应时，通过标签找到转向的请求实例，通过kioctx通知应用程序。

blk-switch应用程序转向实现： 当应用程序转向时，blk-switch通过调用sched_setaffinity内核函数来执行核心移动。移动后，由转向的应用程序生成的请求将提交到新核心的入口队列。blk-switch通过维护每核平均负载实现应用程序转向。即使在应用程序移动过程中存在“正在进行的”请求，blk-switch通过标签将这些请求转发到正确的应用程序。通过找到原始kioctx，它能够唤醒关联的应用程序，使得响应传递到正确的应用程序。


****************************************************


要在Linux内核中实现blk-switch，需要在涉及块层、blk-mq以及其他相关组件的各个文件中进行更改。以下是可能需要探索和修改的一些关键领域：

块层文件：

block/blk-core.c：核心块层功能，包括请求处理。
block/blk-mq.c：多队列块层实现。
block/blk-mq-sched.c：blk-mq队列的调度器。
block/blk-settings.c：块层设置和配置。
blk-mq文件：

block/blk-mq-tag.c：blk-mq的标签管理。
block/blk-mq-cpumap.c：blk-mq的CPU映射。
block/blk-mq-sysfs.c：blk-mq的Sysfs接口。
block/blk-mq-debugfs.c：blk-mq的Debugfs支持。
设备驱动文件：

drivers/block/：包含各种块设备驱动程序的目录。根据目标设备，您可能需要修改或扩展相关的驱动程序。
核心内核文件：

kernel/sched/core.c：Linux内核调度程序，如果您正在实现核心调度或其他与CPU相关的功能，则可能与之相关。
内核API文件：

include/linux/blk-mq.h：定义blk-mq API和结构的头文件。
include/linux/blk_types.h：定义块层类型和结构的头文件。


*******************************************
实现blk-switch的详细步骤将涉及对Linux内核的深入了解和修改多个关键组件。以下是可能需要在提到的一些文件中执行的一般性操作的概述：

1. **块层文件：**
   - `block/blk-core.c`：在这里，可能需要修改请求处理的核心逻辑，以支持blk-switch的请求转发和处理。
   - `block/blk-mq.c`：blk-switch建立在blk-mq之上，这里可能需要扩展以支持blk-switch的队列和调度逻辑。
   - `block/blk-mq-sched.c`：如果blk-switch使用了不同的调度策略，可能需要在这里进行修改。

2. **blk-mq文件：**
   - `block/blk-mq-tag.c`：blk-switch可能需要自定义标签管理逻辑，以便请求可以从一个核心的队列传递到另一个核心的队列。
   - `block/blk-mq-cpumap.c`：考虑到blk-switch的多核心支持，可能需要扩展CPU映射的逻辑。
   - `block/blk-mq-sysfs.c` 和 `block/blk-mq-debugfs.c`：如果需要，可以添加Sysfs接口和Debugfs支持，以便在运行时监视和配置blk-switch。

3. **设备驱动文件：**
   - 根据您的目标设备，需要在 `drivers/block/` 目录中的相应驱动程序中进行修改或扩展。这可能包括本地存储和远程存储的不同逻辑。

4. **核心内核文件：**
   - `kernel/sched/core.c`：如果blk-switch需要特定的核心调度支持，可能需要在这里进行修改。

5. **内核API文件：**
   - `include/linux/blk-mq.h` 和 `include/linux/blk_types.h`：如果blk-switch引入了新的API或数据结构，您需要更新这些头文件。

6. **文档：**
   - `Documentation/block/`：如果对blk-switch的实现进行了文档化，建议更新或创建相应的文档。

具体的实现步骤和代码更改将取决于blk-switch的具体设计和论文中提到的详细内容。




